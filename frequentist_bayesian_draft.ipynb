{
 "metadata": {
  "name": "",
  "signature": "sha256:917fcc755ab75996a4e258971baaf3c3c82152353c38dd0fe369774d0fc7315e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Density Estimation: Frequentist and Bayesian Perspectives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This IPython notebook briefly explains some of the differences between frequentist and bayesian viewpoints, and uses parameter estimation for the Gaussian distribution as an example of the difference between the methods in practice. I originally wrote a version of this for myself in order to better understand the difference between the two schools of thought; while I had an intuitive idea of how each worked, it took me some time to really internalize how the two methods were used in practice. This is intended to do just that; summarize the key ideas of each side and show an example of how a frequentist and a bayesian might solve the same problem respectively.\n",
      "\n",
      "In order to get the most of this, I would recommend that you have a basic understanding of single-variable differentiation, familiarity with the normal distribution, and a working knowledge of Python. I'll be doing my best to include sources where possible, particularly links to primary sources. This topic is incredibly expansive; this guide is aimed at clarifying the basic ideas of each viewpoint, with the understanding that the interested reader can find a more comprehensive explanation both in the provided sources and in the \"Further Reading\" section down the page."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "I. What are the frequentist and bayesian points of view?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Interpretation of Probability"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Broadly speaking, it will help to describe the general, more \"philosophical\" aspects of frequentist and bayesian ideas. Each one rests upon a different interpretation of probability, which I'll describe now.\n",
      "\n",
      "Under the paradigm of [frequentist interpretation of probability](http://en.wikipedia.org/wiki/Frequentist_probability), we make inferences with reference to a specific idea of what a given probability means. In the frequentist view, a probability expresses the fraction of times that an event occurs, out of all possible occurrences. \n",
      "\n",
      "The paradigm of [Bayesian probability](http://en.wikipedia.org/wiki/Bayesian_probability) takes a different view of the fundamental meaning of a probability. For a bayesian, a probability expresses a subjective level of confidence that a particular outcome will actually occur.\n",
      "\n",
      "Having said all that, consider the probability of an event occurring. We can write this in the familiar way, \n",
      "$$P(e) = k$$\n",
      "As you likely know, this is usually written in English as \"the probability of the event e is k\", where e is a possible outcome and k is a real number between 0 and 1.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Frequentist and Bayesian inference"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we get to that, though, we should describe what we mean by inference. \"Inference\" is itself a little bit vague; for our purposes, it's enough to say that statistical inference is \n",
      "\n",
      "[frequentist inference](http://en.wikipedia.org/wiki/Frequentist_inference)\n",
      "\n",
      "[basyesian inference](http://en.wikipedia.org/wiki/Bayesian_inference)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "II. Frequentist and Bayesian Parameter estimation methods: Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**The frequentist parameter estimation procedure** that we will use is called a [Maximum Likelihood Estimate](http://en.wikipedia.org/wiki/Maximum_likelihood), commonly abbreviated MLE. The technique is very commonly used for parameter estimation, and dates back to at least Lagrange [1].\n",
      "\n",
      "Essentially, we have a number of data points, and a model of that data which takes some parameters - our goal is to find a particular value of those parameters which maximizes the likelihood function (in technical terms, this search for a specific value is called a [point estimate](http://en.wikipedia.org/wiki/Point_estimate)).\n",
      "\n",
      "What is the value of the model parameters most likely to have produced the observed data?\n",
      "\n",
      "**The Bayesian procedure** is a little bit trickier to pin down. It is not simply a matter of using a different technique to find a value for the parameters; it actually treats the parameters differently. For a Bayesian, a point estimate is insufficient, for a number of reasons; however, there are two main ones.\n",
      "\n",
      "    1.\n",
      "    2.\n",
      "    \n",
      "In the Bayesian point of view, we aren't even looking for a particular value of the parameters. Instead, we are looking for a **probability distribution** over the parameters.\n",
      "\n",
      "How can we quantify our uncertainty about the value of the parameters, based on both our prior knowledge of the parameters and the observed data?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "III. Example: Gaussian parameter fitting"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The problem of parameter estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Frequentist Method: Maximum Likelihood Estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We first define the Gaussian distribution, for a given mean \\mu and a variance \\sigma:\n",
      "$$N_{\\mu, \\sigma^2} = \\frac{1}{\\sigma \\sqrt{2 \\pi}}$$\n",
      "From this, we construct the likelihood function for a dataset of n observations: \n",
      "$$L_{\\mu, \\sigma^2} = \\Pi_x N_{\\mu, \\sigma^2}(x_i)$$\n",
      "For analytical convenience, we take the log of the likelihood function:\n",
      "$$ln L_{\\mu, \\sigma^2} = -\\frac{n}{2} ln(2 \\pi) - \\frac{n}{2}ln(\\sigma^2) - \\frac{1}{2 \\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Real-life data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have a method for density estimation, it's time to actually fit a distribution to some real-life data.\n",
      "\n",
      "We're going to start by considering the following dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = [1,2,3,4,5,6]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've got that, we can actually get down to estimating the parameters of the data's distribution. This is actually really easy - we did all the hard work in the derivation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample_mean = sum(data)/len(data)\n",
      "sample_sdev = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And then, we simply construct a function which models the distribution, as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Additionally, it becomes very easy to sample from the distribution defined by our parameters:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import gauss\n",
      "[gauss(sample_mean, sample_sdev) for i in range(10)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[5.236916263465567,\n",
        " 3.133461686219703,\n",
        " 3.36635615346968,\n",
        " 3.219955869360218,\n",
        " 2.424273583506911,\n",
        " 3.622717141648797,\n",
        " 2.280188429716906,\n",
        " 2.1546517638584017,\n",
        " 2.7194719206134703,\n",
        " 2.288402566097285]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bayesian Parameter Estimation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named pymc",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-17-5f262cfcb99b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpymc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mImportError\u001b[0m: No module named pymc"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "IV. Discussion: The Great Debate"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That was all very exciting, wasn't it? I still haven't caught my breath, in fact.\n",
      "\n",
      "Let's slow down for a minute to consider the bigger picture. It might seem that the Bayesian approach is in a lot of ways superior to its frequentist counterpart.\n",
      "\n",
      "That begs the question - why aren't we all bayesians? Why aren't frequentist methods relegated to the history books, and why haven't the adherents of the cult of Bayes stormed the Ivory Towers of the world and proclaimed the new kingdom of Bayesian thought?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "V. The End"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I hope you enjoyed reading this as much as I enjoyed writing it. If you have questions, comments, or suggestions feel free to send me an email at louiscialdella [at sign] gmail.com."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[1] The Epic Story of Maximum Likelihood - http://arxiv.org/pdf/0804.2996.pdf\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "VI. Further Reading"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you've come this far, you're probably looking for a more comprehensive look at the methods I'm describing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Notes made while editing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reading:\n",
      "\n",
      "-Why isn't everyone a Bayesian?  - http://www.stat.duke.edu/courses/Spring08/sta122/Handouts/EfronWhyEveryone.pdf\n",
      "\n",
      "-Wasserman all of statistics\n",
      "\n",
      "-Yudkowsy - Bayes theorem - http://yudkowsky.net/rational/bayes\n",
      "\n",
      "-Statistical inference - the big picture - http://www.stat.cmu.edu/~kass/papers/bigpic.pdf\n",
      "\n",
      "-lecture notes on parametric density estimation - http://isites.harvard.edu/fs/docs/icb.topic539621.files/lec16.pdf\n",
      "\n",
      "-fisher vs NP - http://stats.org.uk/statistical-inference/Lenhard2006.pdf\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}