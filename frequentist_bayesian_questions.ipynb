{
 "metadata": {
  "name": "",
  "signature": "sha256:4d2d507fb0ce0058f720ee67ee055089e71c5a8b8370fdec3d294bf4a3604811"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Frequentist and Bayesian Perspectives in statistical inference"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How to read this"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This IPython notebook briefly explains some of the differences between frequentist and bayesian viewpoints in statistical inference. My goal is to start with an intuitive description of the fundamental assumptions on each side, and then show how those assumptions lead to meaningfully different procedures for inference.\n",
      "\n",
      "Each sections contains a basic question about probability and inference; I'll start by clarifying the question and providing a little bit of background. Then, I'll present some possible ways that a frequentist and a bayesian might answer the question respectively. The questions begin very generally, and get more specific as we zero in on our particular example.\n",
      "\n",
      "A general outline is as follows: I'll start by explaining each side's viewpoint on just what probability and inference are. In order to see how the inference procedures work in practice, I'll follow with the consideration of a classical problem, parameter estimation for the Gaussian distribution.\n",
      "\n",
      "I originally wrote an early version of this notebook for myself, in order to better understand the difference between the two schools of thought. While I had an intuitive idea of how each worked, it took me some time to really internalize how the two methods were used in practice. This is intended to do just that: summarize the key ideas of each side and show an example of how a frequentist and a bayesian might solve the same problem respectively.\n",
      "\n",
      "In order to get the most of this, I would recommend that you have a basic understanding of single-variable differentiation (a college calculus course will likely suffice), familiarity with the normal distribution and basic statistics (similarly, a college intro statistics course should give you the needed background), and a working knowledge of Python. I'll be doing my best to include sources where possible, particularly links to primary sources. This topic is incredibly expansive; this guide is aimed at clarifying the basic ideas of each viewpoint, with the understanding that the interested reader can find a more comprehensive explanation both in the provided sources and in the \"Further Reading\" section down the page.\n",
      "\n",
      "[Note on notation: Only where necessary. Intended audience has intuitive grasp, but not necessarily technical knowledge.]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What do probabilities represent?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you've likely heard before, a probability function is applied over a set of possible events in which we are interested - the outcomes of a dice roll, for example. In the case of a dice roll, each of the numbers 1-6 is an event, and a particular roll of the dice is called a trial. In the simplest sense, **a probability function maps an event to a real number**, with certain constraints. If we consider an event \"e\", and a real number \"r\", we can write this in the familiar way as \n",
      "$$P(e) = r$$\n",
      "As a matter of nomenclature, we often refer to P(e) as the \"the probability of e\". However, the values of \"e\" and \"r\" are cannot be arbitrary. The basic axioms of probability generally prescribe a few things:\n",
      "\n",
      "1. For any possible event, the probability of that event is non-negative. That is, for any event e, $$P(e) \\ge 0$$\n",
      "2. The sum of probabilities for all possible events is exactly 1. If we call the set of all possible events \"E\", we can write this as $$\\sum_{e \\in E} P(e) = 1$$\n",
      "\n",
      "You might recognize these as the first two of the [Kolmogorov axioms](http://en.wikipedia.org/wiki/Probability_axioms), which are fundamental to the theoretical development of probability; there is a third axiom, but it won't enter our discussion.\n",
      "\n",
      "The mapping from the space of all events to their probabilities is known as a probability distribution. The axioms tell us what a probability distribution can and cannot be; however, they do not give us any idea what exactly the probability means, where it comes from, or how it is attached to reality. In short, it doesn't tell us anything about the interpretation of the probability function. This issue of interpretation is the first point on which the frequentists and bayesians differ.\n",
      "\n",
      "For **Frequentists, the probability represents the average fraction of trials in which the event will occur in the long run.** Let's say that, as above, we have an event \"e\" which occurs with probability \"r\". Then, over the course of *n* trials, we can expect the event to occur in *about* nr of those trials. Note the \"about\" - there is no guarantee that the number of occurrences of the event will be precisely nr. However, we can expect that as the number of trials gets to be very large, the distribution of events will converge to the true possibility. This idea is formalized in (the law of large numbers)[http://en.wikipedia.org/wiki/Law_of_large_numbers]\n",
      "\n",
      "For **Bayesians, the probability of an event represents the level of certainty that the event will occur.**\n",
      "\n",
      "In the Bayesian view, the probability is not tied to the long-run frequencies explicitly; the probability is essentially an epistemological claim. This has a few major advantages. The most prominent of these is that it allows us to assign probabilities to events which cannot be repeated. Consider, for example, the possibility of the polar ice caps melting in the next ten years - it is evident that the \"long run frequency\" becomes a rather tricky concept with an event which can only occur once, if it even occurs at all.\n",
      "\n",
      "However, this interpretation is necessarily vague. From the statement above, it is not obvious just who has this level of certainty - there is some disagreement between Bayesians as to whether or not Bayesian statistics and inference should be formulated in objective or subjective terms. The technical difference between these formulations requires us to consider Bayes rule, which is covered in the section about inference procedures."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What do the data points represent?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Virtually all statistical analysis is useful specifically because it allows us to extract information from data. But how are we to interpret the data? How does it fit into the larger probabilistic framework? These questions are an important bridge between the previous section and the development of an inferential procedure, as they force us to clarify our assumptions about how the data is structured. This section is relatively short, and is here primarily to lay the philosophical groundwork for the more technical section which follows.\n",
      "\n",
      "For **Frequentists, a set of data is a sample from a much larger population.** A sample is seen as a small subset of a much larger finite (or even infinite) population. The hope is that the smaller sample has similar properties to the larger population from which it was taken. Under that assumption, we can examine the subset in order to glean information about the larger subset.\n",
      "\n",
      "For **Bayesians, a set of data is experimental evidence which increases or decreases our certainty in a particular outcome.** We begin with a hypothesis about something, and we use the data as evidence for or against our theory."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How can we infer information about statistical models from the data?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. What parameters of a statistical model can we infer? What form does this inference take?\n",
      "2. One frequentist technique of making point estimates about model parameters is the maximum likelihood method. A good hypothesis does a good job of explaining the observed phenomenon under the frequentist paradigm.\n",
      "3. We can quantify our uncertainty about the strength of a hypothesis as a probability distribution."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How can we make inferences about the parameters of the Gaussian?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. How does the data enable us to form hypotheses about the average and standard deviation?\n",
      "2. The MLE allows us to find the specific values for the parameters which best explain the sample data, and correspond to the true values of those parameters in as much as the sample is representative.\n",
      "3. We begin by assigning a prior distribution which encodes our beliefs about the parameters, and then we update that distribution based on the observed data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How can we make these inferences automatically?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. How can we write a program which will automatically produce hypotheses about the data?\n",
      "2. We can derive the MLE for the parameters of the gaussian, and apply the derived formulae to the sample data.\n",
      "3. We can model the prior distribution and update it to obtain the posterior; we can then sample from the posterior."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What are the drawbacks of each method?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Why isn't everyone a frequentist? Why isn't everyone a Bayesian?\n",
      "2. Maximum likelihood is highly dependent on the assumption that the sample is representative, and the estimator itself has very high variance (in particular, MLE for Gaussian parameters is not robust to outliers). \n",
      "3. The choice of prior is sensitive and not at all obvious; the calculations are laborious."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The End"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Further Reading"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://www.austincc.edu/mparker/stat/nov04/talk_nov04.pdf\n",
      "\n",
      "http://www.stat.cmu.edu/~rsteorts/btheory/goldstein_subjective_2006.pdf\n",
      "\n",
      "http://www.stat.duke.edu/~berger/papers/obayes-debate.pdf\n",
      "\n",
      "http://www.stat.cmu.edu/~kass/papers/bigpic.pdf\n",
      "\n",
      "http://www.stat.duke.edu/courses/Spring08/sta122/Handouts/EfronWhyEveryone.pdf\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}